@inproceedings{10.1145/3617232.3624858,
author = {Xia, Chunwei and Zhao, Jiacheng and Sun, Qianqi and Wang, Zheng and Wen, Yuan and Yu, Teng and Feng, Xiaobing and Cui, Huimin},
title = {Optimizing Deep Learning Inference via Global Analysis and Tensor Expressions},
year = {2024},
isbn = {9798400703720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617232.3624858},
doi = {10.1145/3617232.3624858},
abstract = {Optimizing deep neural network (DNN) execution is important but becomes increasingly difficult as DNN complexity grows. Existing DNN compilers cannot effectively exploit optimization opportunities across operator boundaries, leaving room for improvement. To address this challenge, we present Souffle, an open-source compiler that optimizes DNN inference across operator boundaries. Souffle creates a global tensor dependency graph using tensor expressions, traces data flow and tensor information, and partitions the computation graph into subprograms based on dataflow analysis and resource constraints. Within a subprogram, Souffle performs local optimization via semantic-preserving transformations, finds an optimized program schedule, and improves instruction-level parallelism and data reuse. We evaluated Souffle using six representative DNN models on an NVIDIA A100 GPU. Experimental results show that Souffle consistently outperforms six state-of-the-art DNN optimizers by delivering a geometric mean speedup of up to 3.7\texttimes{} over TensorRT and 7.8\texttimes{} over Tensorflow XLA.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
pages = {286â€“301},
numpages = {16},
keywords = {deep neural network, compiler optimization, tensor expression, GPU},
location = {La Jolla, CA, USA},
series = {ASPLOS '24}
}